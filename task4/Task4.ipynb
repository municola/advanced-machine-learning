{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mice Sleep Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.fft import fft, fftshift\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal as sig\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eeg1 = pd.read_csv('train_eeg1.csv')\n",
    "train_eeg2 = pd.read_csv('train_eeg2.csv')\n",
    "train_emg = pd.read_csv('train_emg.csv')\n",
    "test_eeg1 = pd.read_csv('test_eeg1.csv')\n",
    "test_eeg2 = pd.read_csv('test_eeg2.csv')\n",
    "test_emg = pd.read_csv('test_emg.csv')\n",
    "train_labels = pd.read_csv('train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eeg1 = train_eeg1.drop(['Id'],axis=1)\n",
    "train_eeg2 = train_eeg2.drop(['Id'],axis=1)\n",
    "train_emg = train_emg.drop(['Id'],axis=1)\n",
    "test_eeg1 = test_eeg1.drop(['Id'],axis=1)\n",
    "test_eeg2 = test_eeg2.drop(['Id'],axis=1)\n",
    "test_emg = test_emg.drop(['Id'],axis=1)\n",
    "train_labels = train_labels.drop(['Id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (Train)\n",
    "\n",
    "Mouse 1,2,3 = Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 3 mice\n",
    "stop1 = 21600\n",
    "stop2 = 43200\n",
    "stop3 = 64800\n",
    "\n",
    "m1_eeg1 = train_eeg1[0:stop1]\n",
    "m1_eeg2 = train_eeg2[0:stop1]\n",
    "m1_emg = train_emg[0:stop1]\n",
    "m1_labels = train_labels[0:stop1]\n",
    "\n",
    "m2_eeg1 = train_eeg1[stop1:stop2]\n",
    "m2_eeg2 = train_eeg2[stop1:stop2]\n",
    "m2_emg = train_emg[stop1:stop2]\n",
    "m2_labels = train_labels[stop1:stop2]\n",
    "\n",
    "m3_eeg1 = train_eeg1[stop2:stop3]\n",
    "m3_eeg2 = train_eeg2[stop2:stop3]\n",
    "m3_emg = train_emg[stop2:stop3]\n",
    "m3_labels = train_labels[stop2:stop3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(df):\n",
    "    firstSample = pd.DataFrame(df.iloc[0]).T\n",
    "    lastSample = pd.DataFrame(df.iloc[-1]).T\n",
    "    df = pd.concat([firstSample,df,lastSample], axis=0, join='outer', ignore_index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_eeg1 = padding(m1_eeg1)\n",
    "m1_eeg2 = padding(m1_eeg2)\n",
    "m1_emg = padding(m1_emg)\n",
    "\n",
    "m2_eeg1 = padding(m2_eeg1)\n",
    "m2_eeg2 = padding(m2_eeg2)\n",
    "m2_emg = padding(m2_emg)\n",
    "\n",
    "m3_eeg1 = padding(m3_eeg1)\n",
    "m3_eeg2 = padding(m3_eeg2)\n",
    "m3_emg = padding(m3_emg)\n",
    "\n",
    "assert m1_eeg1.shape == (21602,512)\n",
    "assert m2_eeg1.shape == (21602,512)\n",
    "assert m3_eeg1.shape == (21602,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step a): Fourier Transformation with Hamming windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftHamming(signal):\n",
    "    '''\n",
    "    @return: 24 numbers for that window\n",
    "    '''\n",
    "    \n",
    "    # Apply Hamming Window\n",
    "    assert signal.shape == (256,)\n",
    "    window = np.hamming(256)\n",
    "    signal = signal*window\n",
    "    assert signal.shape == (256,)\n",
    "    \n",
    "    # Apply FFT\n",
    "    fftResult = fft(signal, 128)\n",
    "    \n",
    "    # Calculate PSD\n",
    "    psd = np.square(np.absolute(fftResult))\n",
    "\n",
    "    return psd\n",
    "    \n",
    "def slidingWindow(df):\n",
    "    '''\n",
    "    @return: 24\n",
    "    '''\n",
    "    allValues = []\n",
    "    \n",
    "    ts = df.to_numpy().flatten()\n",
    "    i = 385 # 512 - 127 (127 bc. we chose hamming window to be center in middle (chose left of the two middle elements))\n",
    "    k = 0\n",
    "    \n",
    "    pbar = tqdm(position=0, leave=True, total = 21600*32)\n",
    "    while(i < 385+21600*512):\n",
    "        pbar.update(1)\n",
    "        k += 1\n",
    "        \n",
    "        # Get the 24 Values for this window\n",
    "        values = fftHamming(ts[i:i+256])\n",
    "        \n",
    "        allValues.append(values)\n",
    "        i+=16\n",
    "    pbar.close()\n",
    "    assert int(k) == int((21600*32))\n",
    "    \n",
    "    return allValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691200/691200 [00:09<00:00, 72001.35it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 73151.38it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 72451.46it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 72670.50it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 73674.13it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 72281.74it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 71791.47it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 71017.24it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 71832.08it/s]\n"
     ]
    }
   ],
   "source": [
    "m1_eeg1_a = slidingWindow(m1_eeg1)\n",
    "m1_eeg2_a = slidingWindow(m1_eeg2)\n",
    "m1_emg_a = slidingWindow(m1_emg)\n",
    "\n",
    "m2_eeg1_a = slidingWindow(m2_eeg1)\n",
    "m2_eeg2_a = slidingWindow(m2_eeg2)\n",
    "m2_emg_a = slidingWindow(m2_emg)\n",
    "\n",
    "m3_eeg1_a = slidingWindow(m3_eeg1)\n",
    "m3_eeg2_a = slidingWindow(m3_eeg2)\n",
    "m3_emg_a = slidingWindow(m3_emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step b): Apply Bandpass Filtering or Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpassFilter(signal):\n",
    "    newsignal = []\n",
    "    i = 1\n",
    "    while i <= 48:\n",
    "        newsignal.append(signal[i]+signal[i+1])\n",
    "        i+=2\n",
    "    assert len(newsignal) == 24\n",
    "    return newsignal\n",
    "\n",
    "def applyBandpassFilter(ts):\n",
    "    newsignal = []\n",
    "    for el in ts:\n",
    "        newsignal.append(bandpassFilter(el))\n",
    "        \n",
    "    return newsignal\n",
    "\n",
    "def applyIntegration(ts):\n",
    "    newsignal = []\n",
    "    mysum = 0\n",
    "    for el in ts:\n",
    "        mysum = np.sum(el[1:61])\n",
    "        tempsignal = []\n",
    "        for i in range(0,24):\n",
    "            tempsignal.append(mysum)\n",
    "        newsignal.append(tempsignal)\n",
    "    \n",
    "    assert len(newsignal) == len(ts)\n",
    "    return newsignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse1 finished\n",
      "mouse2 finished\n",
      "mouse3 finished\n"
     ]
    }
   ],
   "source": [
    "m1_eeg1_b = applyBandpassFilter(m1_eeg1_a)\n",
    "m1_eeg2_b = applyBandpassFilter(m1_eeg2_a)\n",
    "m1_emg_b = applyIntegration(m1_emg_a) # Integration\n",
    "print('mouse1 finished')\n",
    "\n",
    "m2_eeg1_b = applyBandpassFilter(m2_eeg1_a)\n",
    "m2_eeg2_b = applyBandpassFilter(m2_eeg2_a)\n",
    "m2_emg_b = applyIntegration(m2_emg_a) # Integration\n",
    "print('mouse2 finished')\n",
    "\n",
    "m3_eeg1_b = applyBandpassFilter(m3_eeg1_a)\n",
    "m3_eeg2_b = applyBandpassFilter(m3_eeg2_a)\n",
    "m3_emg_b = applyIntegration(m3_emg_a) # Integration\n",
    "print('mouse3 finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step c): Take Logarithm and standartize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse1 finished\n",
      "mouse2 finished\n",
      "mouse3 finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "m1_eeg1_c = preprocessing.scale(np.log(np.array(m1_eeg1_b)))\n",
    "m1_eeg2_c = preprocessing.scale(np.log(np.array(m1_eeg2_b)))\n",
    "m1_emg_c = preprocessing.scale(np.log(np.array(m1_emg_b)))\n",
    "print('mouse1 finished')\n",
    "\n",
    "m2_eeg1_c = preprocessing.scale(np.log(np.array(m2_eeg1_b)))\n",
    "m2_eeg2_c = preprocessing.scale(np.log(np.array(m2_eeg2_b)))\n",
    "m2_emg_c = preprocessing.scale(np.log(np.array(m2_emg_b)))\n",
    "print('mouse2 finished')\n",
    "\n",
    "m3_eeg1_c = preprocessing.scale(np.log(np.array(m3_eeg1_b)))\n",
    "m3_eeg2_c = preprocessing.scale(np.log(np.array(m3_eeg2_b)))\n",
    "m3_emg_c = preprocessing.scale(np.log(np.array(m3_emg_b)))\n",
    "print('mouse3 finished')\n",
    "\n",
    "assert m1_eeg1_c.shape == (691200, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring it into Training shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding2(df):\n",
    "    df_end = np.insert(df,0,df[0:32],axis=0)\n",
    "    df_end = np.insert(df_end,0,df[0:32],axis=0)\n",
    "    df_end = np.append(df_end, df[-32:],axis=0)\n",
    "    df_end = np.append(df_end, df[-32:],axis=0)\n",
    "\n",
    "    return df_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainSamples(df):\n",
    "    i = 0\n",
    "    df_end = []\n",
    "    while i <= (691328 - 5*32):\n",
    "        df_end.append(df[i:i+32*5])\n",
    "        i += 32\n",
    "    return df_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse1 finished\n",
      "mouse2 finished\n",
      "mouse3 finished\n"
     ]
    }
   ],
   "source": [
    "m1_eeg1_end = np.array(createTrainSamples(padding2(m1_eeg1_c)))\n",
    "m1_eeg2_end = np.array(createTrainSamples(padding2(m1_eeg2_c)))\n",
    "m1_emg_end = np.array(createTrainSamples(padding2(m1_emg_c)))\n",
    "print('mouse1 finished')\n",
    "\n",
    "m2_eeg1_end = np.array(createTrainSamples(padding2(m2_eeg1_c)))\n",
    "m2_eeg2_end = np.array(createTrainSamples(padding2(m2_eeg2_c)))\n",
    "m2_emg_end = np.array(createTrainSamples(padding2(m2_emg_c)))\n",
    "print('mouse2 finished')\n",
    "\n",
    "m3_eeg1_end = np.array(createTrainSamples(padding2(m3_eeg1_c)))\n",
    "m3_eeg2_end = np.array(createTrainSamples(padding2(m3_eeg2_c)))\n",
    "m3_emg_end = np.array(createTrainSamples(padding2(m3_emg_c)))\n",
    "print('mouse3 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_end = np.stack([m1_eeg1_end,m1_eeg2_end,m1_emg_end], axis=1)\n",
    "m2_end = np.stack([m2_eeg1_end,m2_eeg2_end,m2_emg_end], axis=1)\n",
    "m3_end = np.stack([m3_eeg1_end,m3_eeg2_end,m3_emg_end], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_train = np.concatenate([m1_end,m2_end],axis=0)\n",
    "mice_val = m3_end\n",
    "\n",
    "assert mice_train.shape == (43200,3,160,24)\n",
    "assert mice_val.shape == (21600,3,160,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = np.concatenate([m1_labels, m2_labels], axis=0)\n",
    "labels_val = m3_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = labels_train-1\n",
    "labels_val = labels_val-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 3 mice\n",
    "stop1 = 21600\n",
    "stop2 = 43200\n",
    "\n",
    "m4_eeg1 = test_eeg1[0:stop1]\n",
    "m4_eeg2 = test_eeg2[0:stop1]\n",
    "m4_emg = test_emg[0:stop1]\n",
    "\n",
    "m5_eeg1 = test_eeg1[stop1:stop2]\n",
    "m5_eeg2 = test_eeg2[stop1:stop2]\n",
    "m5_emg = test_emg[stop1:stop2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_eeg1 = padding(m4_eeg1)\n",
    "m4_eeg2 = padding(m4_eeg2)\n",
    "m4_emg = padding(m4_emg)\n",
    "\n",
    "m5_eeg1 = padding(m5_eeg1)\n",
    "m5_eeg2 = padding(m5_eeg2)\n",
    "m5_emg = padding(m5_emg)\n",
    "\n",
    "assert m4_eeg1.shape == (21602,512)\n",
    "assert m5_eeg1.shape == (21602,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step a): Fourier Transformation with Hamming windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 691200/691200 [00:09<00:00, 71197.93it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 71740.42it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 71929.43it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 72524.93it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 72014.38it/s]\n",
      "100%|██████████| 691200/691200 [00:09<00:00, 71729.21it/s]\n"
     ]
    }
   ],
   "source": [
    "m4_eeg1_a = slidingWindow(m4_eeg1)\n",
    "m4_eeg2_a = slidingWindow(m4_eeg2)\n",
    "m4_emg_a = slidingWindow(m4_emg)\n",
    "\n",
    "m5_eeg1_a = slidingWindow(m5_eeg1)\n",
    "m5_eeg2_a = slidingWindow(m5_eeg2)\n",
    "m5_emg_a = slidingWindow(m5_emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step b): Apply Bandpass Filtering or Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse4 finished\n",
      "mouse5 finished\n"
     ]
    }
   ],
   "source": [
    "m4_eeg1_b = applyBandpassFilter(m4_eeg1_a)\n",
    "m4_eeg2_b = applyBandpassFilter(m4_eeg2_a)\n",
    "m4_emg_b = applyIntegration(m4_emg_a) # Integration\n",
    "print('mouse4 finished')\n",
    "\n",
    "m5_eeg1_b = applyBandpassFilter(m5_eeg1_a)\n",
    "m5_eeg2_b = applyBandpassFilter(m5_eeg2_a)\n",
    "m5_emg_b = applyIntegration(m5_emg_a) # Integration\n",
    "print('mouse5 finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step c): Take Logarithm and standartize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse4 finished\n",
      "mouse5 finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "m4_eeg1_c = preprocessing.scale(np.log(np.array(m4_eeg1_b)))\n",
    "m4_eeg2_c = preprocessing.scale(np.log(np.array(m4_eeg2_b)))\n",
    "m4_emg_c = preprocessing.scale(np.log(np.array(m4_emg_b)))\n",
    "print('mouse4 finished')\n",
    "\n",
    "m5_eeg1_c = preprocessing.scale(np.log(np.array(m5_eeg1_b)))\n",
    "m5_eeg2_c = preprocessing.scale(np.log(np.array(m5_eeg2_b)))\n",
    "m5_emg_c = preprocessing.scale(np.log(np.array(m5_emg_b)))\n",
    "print('mouse5 finished')\n",
    "\n",
    "assert m4_eeg1_c.shape == (691200, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring it into Training shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse4 finished\n",
      "mouse5 finished\n"
     ]
    }
   ],
   "source": [
    "m4_eeg1_end = np.array(createTrainSamples(padding2(m4_eeg1_c)))\n",
    "m4_eeg2_end = np.array(createTrainSamples(padding2(m4_eeg2_c)))\n",
    "m4_emg_end = np.array(createTrainSamples(padding2(m4_emg_c)))\n",
    "print('mouse4 finished')\n",
    "\n",
    "m5_eeg1_end = np.array(createTrainSamples(padding2(m5_eeg1_c)))\n",
    "m5_eeg2_end = np.array(createTrainSamples(padding2(m5_eeg2_c)))\n",
    "m5_emg_end = np.array(createTrainSamples(padding2(m5_emg_c)))\n",
    "print('mouse5 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_end = np.stack([m4_eeg1_end,m4_eeg2_end,m4_emg_end], axis=1)\n",
    "m5_end = np.stack([m5_eeg1_end,m5_eeg2_end,m5_emg_end], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_test = np.concatenate([m4_end,m5_end],axis=0)\n",
    "assert mice_test.shape == (43200,3,160,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif,mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from pytorch_lightning.metrics.functional.classification import stat_scores_multiple_classes as ssmc\n",
    "from pytorch_lightning.metrics.functional.classification import multiclass_roc as mc\n",
    "from pytorch_lightning.metrics.functional import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        permutation = torch.randperm(43200)\n",
    "        self.X_train = torch.from_numpy(mice_train).float()\n",
    "        self.X_train = self.X_train[permutation]\n",
    "        self.y_train = torch.from_numpy(labels_train)\n",
    "        self.y_train = self.y_train[permutation]\n",
    "        self.n_samples = self.X_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_train[index], self.y_train[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        permutation = torch.randperm(21600)\n",
    "        self.X_val = torch.from_numpy(mice_val).float()\n",
    "        self.X_val = self.X_val[permutation]\n",
    "        self.y_val = torch.from_numpy(labels_val)\n",
    "        self.y_val = self.y_val[permutation]\n",
    "        self.n_samples = self.X_val.shape[0]     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_val[index], self.y_val[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.X_test = torch.from_numpy(mice_test).float()\n",
    "        self.n_samples = self.X_test.shape[0]     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_test[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "class NiciNet(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.maxPool1 = nn.MaxPool2d((2, 3), stride=(2, 3))\n",
    "        self.CNN = nn.Conv2d(3, 50, (3, 3), stride=(1, 1))\n",
    "        self.maxPool2 = nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "        self.dense1 = nn.Linear(50*39*3,1000)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(1000,1000)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dense3 = nn.Linear(1000,3)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        (B,3,160,24)\n",
    "        '''\n",
    "        B = x.shape[0]\n",
    "        assert x.shape == (B,3,160,24)\n",
    "        x = self.maxPool1(x)\n",
    "        assert x.shape == (B,3,80,8)\n",
    "        \n",
    "        x = self.CNN(x)\n",
    "        x = F.relu(x)\n",
    "        assert x.shape == (B,50,78,6)\n",
    "        \n",
    "        x = self.maxPool2(x)\n",
    "        assert x.shape == (B,50,39,3)\n",
    "        \n",
    "        x = x.view(-1,50*39*3)\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.trainDataset = TrainDataset()\n",
    "        self.validationDataset = ValidationDataset()\n",
    "        self.testDataset = TestDataset() \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainDataset, batch_size=100, num_workers=7)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validationDataset, batch_size=21600)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testDataset,batch_size=1)\n",
    "                         \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-5)\n",
    "        return optimizer\n",
    "    \n",
    "    def myloss(self, logits, labels):\n",
    "        labels = labels.squeeze(dim=1)\n",
    "                \n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        l2 = 0\n",
    "        \n",
    "        for l in labels:\n",
    "            if (l == 0):\n",
    "                l0 += 1\n",
    "            if (l == 1):\n",
    "                l1 += 1\n",
    "            if (l == 2):\n",
    "                l2 += 1\n",
    "                \n",
    "        weight0 = 0 \n",
    "        weight1 = 0\n",
    "        weight2 = 0\n",
    "        \n",
    "        if (l0 == 0):\n",
    "            weight0 = 0\n",
    "        else:\n",
    "            weight0 = 1/l0\n",
    "        if (l1 == 0):\n",
    "            weight1 = 0\n",
    "        else:\n",
    "            weight1 = 1/l1\n",
    "        if (l2 == 0):\n",
    "            weight2 = 0\n",
    "        else:\n",
    "            weight2 = 1/l2\n",
    "                \n",
    "        lossfn = nn.NLLLoss(weight=torch.tensor([weight0,weight1,weight2]).cuda())        \n",
    "        loss = lossfn(logits,labels)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.myloss(logits, y)\n",
    "        \n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.myloss(logits, y)\n",
    "\n",
    "        # Calcualte BMAC\n",
    "        winners = torch.argmax(logits, dim=1).cpu()\n",
    "        print(balanced_accuracy_score(y.cpu(),winners))\n",
    "        self.accs.append(balanced_accuracy_score(y.cpu(),winners))\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'val_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_epoch_end(self, logs):\n",
    "        avg = np.mean(np.array(self.losses))\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | maxPool1 | MaxPool2d  | 0     \n",
      "1 | CNN      | Conv2d     | 1.4 K \n",
      "2 | maxPool2 | MaxPool2d  | 0     \n",
      "3 | dense1   | Linear     | 5.9 M \n",
      "4 | dropout1 | Dropout    | 0     \n",
      "5 | dense2   | Linear     | 1.0 M \n",
      "6 | dropout2 | Dropout    | 0     \n",
      "7 | dense3   | Linear     | 3.0 K \n",
      "8 | dropout3 | Dropout    | 0     \n",
      "9 | softmax  | LogSoftmax | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb34ad6b682843268e50d9b57ccbc122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2315800669045244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbcdc3bc1734639bbcad5a73aa49fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fac95538a342f8ae2c54084a166e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9515024410268463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409005c988ac4d328c817e399bff0271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9517424633473156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4bde472aca48f193e3be041ba063a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522616506682868\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NiciNet()\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=6,check_val_every_n_epoch=2)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(torch.from_numpy(mice_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = torch.argmax(preds, dim=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = np.array(winners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "\n",
    "for i in range(0,43200):\n",
    "    if winners[i] == 0:\n",
    "        a += 1\n",
    "    if winners[i] == 1:\n",
    "        b += 1\n",
    "    if winners[i] == 2:\n",
    "        c += 1\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change to 1,2,3 from 0,1,2 (class labels)\n",
    "winners += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_handin = pd.read_csv('sample.csv')\n",
    "y_handin['y'] = winners\n",
    "y_handin.to_csv(r'y_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy on m3 before postprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NiciNet(\n",
       "  (maxPool1): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=0, dilation=1, ceil_mode=False)\n",
       "  (CNN): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (maxPool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (dense1): Linear(in_features=5850, out_features=1000, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dense2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (dense3): Linear(in_features=1000, out_features=3, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_labels = m3_labels - 1\n",
    "my_train_data = m3_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(torch.from_numpy(my_train_data).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = np.array(torch.argmax(preds, dim=1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522616506682868\n"
     ]
    }
   ],
   "source": [
    "print(balanced_accuracy_score(my_train_labels,winners))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainingset MoNet on M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model(torch.from_numpy(m3_end).float())\n",
    "my_labels_train = (m3_labels[5:21595] - 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pred_train.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = []\n",
    "\n",
    "for i in range(5,21595):\n",
    "    sample = []\n",
    "    for j in range(i-5,i+6):\n",
    "        sample.append(pred_train[j][0])\n",
    "        sample.append(pred_train[j][1])\n",
    "        sample.append(pred_train[j][2])\n",
    "    trainingset.append(sample)\n",
    "        \n",
    "trainingset = np.array(trainingset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trainingset.shape[0] == my_labels_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validationset MoNet on M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_val_labels = (m3_labels[5:21595] - 1).to_numpy()\n",
    "my_val_data = m3_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = model(torch.from_numpy(my_val_data).float())\n",
    "pred_val = pred_val.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationset = []\n",
    "for i in range(5,21595):\n",
    "    sample = []\n",
    "    for j in range(i-5,i+6):\n",
    "        sample.append(pred_val[j][0])\n",
    "        sample.append(pred_val[j][1])\n",
    "        sample.append(pred_val[j][2])\n",
    "    validationset.append(sample)\n",
    "        \n",
    "validationset = np.array(validationset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training MoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        permutation = torch.randperm(21590)\n",
    "        self.X_train = torch.from_numpy(trainingset).float()\n",
    "        self.X_train = self.X_train[permutation]\n",
    "        self.y_train = torch.from_numpy(my_labels_train)\n",
    "        self.y_train = self.y_train[permutation]\n",
    "        self.n_samples = self.X_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_train[index], self.y_train[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        permutation = torch.randperm(21590)\n",
    "        self.X_val = torch.from_numpy(validationset).float()\n",
    "        self.X_val = self.X_val[permutation]\n",
    "        self.y_val = torch.from_numpy(my_val_labels)\n",
    "        self.y_val = self.y_val[permutation]\n",
    "        self.n_samples = self.X_val.shape[0]     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_val[index], self.y_val[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "class MoNet(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "\n",
    "        self.dense1 = nn.Linear(33,10)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(10,3)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        (B,3,160,24)\n",
    "        '''\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.trainDataset = TrainDataset()\n",
    "        self.validationDataset = ValidationDataset()\n",
    "        self.testDataset = TestDataset() \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainDataset, batch_size=100, num_workers=7)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validationDataset, batch_size=21590)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testDataset,batch_size=1)\n",
    "                         \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def myloss(self, logits, labels):\n",
    "        labels = labels.squeeze(dim=1)\n",
    "                \n",
    "        l0 = 0\n",
    "        l1 = 0\n",
    "        l2 = 0\n",
    "        \n",
    "        for l in labels:\n",
    "            if (l == 0):\n",
    "                l0 += 1\n",
    "            if (l == 1):\n",
    "                l1 += 1\n",
    "            if (l == 2):\n",
    "                l2 += 1\n",
    "                \n",
    "        weight0 = 0 \n",
    "        weight1 = 0\n",
    "        weight2 = 0\n",
    "        \n",
    "        if (l0 == 0):\n",
    "            weight0 = 0\n",
    "        else:\n",
    "            weight0 = 1/l0\n",
    "        if (l1 == 0):\n",
    "            weight1 = 0\n",
    "        else:\n",
    "            weight1 = 1/l1\n",
    "        if (l2 == 0):\n",
    "            weight2 = 0\n",
    "        else:\n",
    "            weight2 = 1/l2\n",
    "                \n",
    "        lossfn = nn.NLLLoss(weight=torch.tensor([weight0,weight1,weight2]).cuda())        \n",
    "        loss = lossfn(logits,labels)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.myloss(logits, y)\n",
    "        \n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.myloss(logits, y)\n",
    "\n",
    "        # Calcualte BMAC\n",
    "        winners = torch.argmax(logits, dim=1).cpu()\n",
    "        print(balanced_accuracy_score(y.cpu(),winners))\n",
    "        self.accs.append(balanced_accuracy_score(y.cpu(),winners))\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'val_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_epoch_end(self, logs):\n",
    "        avg = np.mean(np.array(self.losses))\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | dense1   | Linear     | 340   \n",
      "1 | dropout1 | Dropout    | 0     \n",
      "2 | dense2   | Linear     | 33    \n",
      "3 | dropout2 | Dropout    | 0     \n",
      "4 | softmax  | LogSoftmax | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ab09d185f7477fa53d12e7ad2cd54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29000298217958503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d5a40b84794f67bd43e3f502bf943b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3682497087495fbb623fab3336d202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9684128226372745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e84181a54834b018e60c175ac38b25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791799162234848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MoNet()\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=4,check_val_every_n_epoch=2)\n",
    "trainer.fit(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return on m3 to see improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoNet(\n",
       "  (dense1): Linear(in_features=33, out_features=10, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dense2): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model2(torch.from_numpy(validationset).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = np.array(torch.argmax(preds, dim=1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791459995229014\n"
     ]
    }
   ],
   "source": [
    "print(balanced_accuracy_score(my_val_labels,winners))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Postprocessing on Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NiciNet(\n",
       "  (maxPool1): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=0, dilation=1, ceil_mode=False)\n",
       "  (CNN): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (maxPool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (dense1): Linear(in_features=5850, out_features=1000, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dense2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (dense3): Linear(in_features=1000, out_features=3, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_m4 = model(torch.from_numpy(m4_end).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_m4 = preds_m4.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NiciNet(\n",
       "  (maxPool1): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=0, dilation=1, ceil_mode=False)\n",
       "  (CNN): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (maxPool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (dense1): Linear(in_features=5850, out_features=1000, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dense2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (dense3): Linear(in_features=1000, out_features=3, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_m5 = model(torch.from_numpy(m5_end).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_m5 = preds_m5.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For samples (5,21595) do postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset1 = []\n",
    "\n",
    "for i in range(5,21595):\n",
    "    sample = []\n",
    "    for j in range(i-5,i+6):\n",
    "        sample.append(preds_m4[j][0])\n",
    "        sample.append(preds_m4[j][1])\n",
    "        sample.append(preds_m4[j][2])\n",
    "    testdataset1.append(sample)\n",
    "        \n",
    "testdataset1 = np.array(testdataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset2 = []\n",
    "\n",
    "for i in range(5,21595):\n",
    "    sample = []\n",
    "    for j in range(i-5,i+6):\n",
    "        sample.append(preds_m5[j][0])\n",
    "        sample.append(preds_m5[j][1])\n",
    "        sample.append(preds_m5[j][2])\n",
    "    testdataset2.append(sample)\n",
    "        \n",
    "testdataset2 = np.array(testdataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoNet(\n",
       "  (dense1): Linear(in_features=33, out_features=10, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dense2): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test1 = model2(torch.from_numpy(testdataset1).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test2 = model2(torch.from_numpy(testdataset2).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace postprocessed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.3841855e-07, -1.5570528e+01, -1.6095476e+01],\n",
       "       [ 0.0000000e+00, -1.6804090e+01, -1.7417183e+01],\n",
       "       [-3.5762781e-07, -1.5415998e+01, -1.6527508e+01],\n",
       "       ...,\n",
       "       [-3.4952323e+00, -2.0757461e-01, -1.8508370e+00],\n",
       "       [-2.8224123e+00, -3.6785606e-01, -1.3930334e+00],\n",
       "       [-2.7304614e+00, -2.4597836e-01, -1.8781569e+00]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners1 = np.argmax(preds_m4, axis=1)\n",
    "winners1[5:21595] = np.array(torch.argmax(preds_test1, dim=1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners2 = np.argmax(preds_m5, axis=1)\n",
    "winners2[5:21595] = np.array(torch.argmax(preds_test2, dim=1).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred = np.concatenate([winners1,winners2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "22154\n",
      "2200\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "\n",
    "for i in range(0,43200):\n",
    "    if finalpred[i] == 1:\n",
    "        a += 1\n",
    "    if finalpred[i] == 2:\n",
    "        b += 1\n",
    "    if finalpred[i] == 3:\n",
    "        c += 1\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_handin = pd.read_csv('sample.csv')\n",
    "y_handin['y'] = finalpred\n",
    "y_handin.to_csv(r'y_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred = pd.read_csv('y_test (1).csv')\n",
    "finalpred = finalpred['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eliminate loners\n",
    "k = 0\n",
    "for i in range (3, finalpred.shape[0]-3):\n",
    "    if finalpred[i-1] == finalpred[i+1] and finalpred[i] != finalpred[i+1]:\n",
    "        finalpred[i] = finalpred[i+1]\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_handin = pd.read_csv('sample.csv')\n",
    "y_handin['y'] = finalpred\n",
    "y_handin.to_csv(r'y_test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
