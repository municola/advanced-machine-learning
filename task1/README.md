Firstly we import the data in R. Then we eliminate all the columns that have less than 4 unique values. We then split the data into a training and validation set. We standardize the Training Data according to its mean and standardeviation an dpply this transformation on the validaation and test set. Then we set all NA's to 0. We do this because our algorithms don't run with NA's. Another possibility would be to impute the missing data. Next we use a Random Forest Regressor to select the imortant features. We set the option 'importance' to True so thaat we see how important each variable is. Then if the importance of a variable is >0.1 we keep it. This leaves us with 71 covarates. Next up we fit a random forest with only these covariates to avoid oferfitting. We use 1000 trees where we include 50 variables in each tree. With this model we find very good results on the validation set and also on the test set. For the last step we decide to rerun our whole process on the entire X_train.csv so thata we use as much data/information as possibleto predict Ytest. With this model we managed to beat the hard baseline.

Final Score: 0.6679
